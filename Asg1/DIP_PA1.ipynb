{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division   # Python 2/3 compatibility\n",
    "from skimage import io, img_as_ubyte,measure\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.filters import *\n",
    "\n",
    "import numpy as np                                # array manipulation package\n",
    "import matplotlib.pylab as plt                 # plotting package\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (7, 16)         # set default figure size\n",
    "plt.rcParams['image.cmap'] = 'gray'               # set default colormap to gray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 : Image Topology and Geometric Transformations\n",
    "\n",
    "The following programming assignment involves two main tasks, viz. operations involving the pixel neighborhood, and geometric transformations.\n",
    "\n",
    "**Please, follow carefully the submission instructions given in the end of this notebook.** You are encouraged to seek information in other places than the course book and lecture material but remember **list all your sources under references**.\n",
    "\n",
    "If you experience problems that you cannot solve using the course material or the Python documentation, or have any questions regarding the programming assignments, please do not hesitate to contact the course assistant by sending an e-mail at dip@unioulu.oulu.fi. You can also join in for the Q & A session (schedule is given on the course page in Moodle) for this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**At first, fill in your personal details below.**\n",
    "\n",
    "# Personal details:\n",
    "\n",
    "* **Name(s) and student ID(s):**\n",
    "* **Contact information:**`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Image topology using pixel adjacency\n",
    "\n",
    "This task relies on the concepts of pixel adjacency, connectivity, regions and boundaries. Specifically, we will be working with 4-adjacency and 8-adjacency of a pixel, and extract regions for these two cases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1. Read `Image1.png` as a color as well as a grayscale image. Next, convert it to a binary image. Print the size of the Numpy array representing the colored image. Display all the three images in the same figure.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the test image and convert it to grayscale as well\n",
    "colored_image = io.imread('Image1.png')\n",
    "grayscale_image = io.imread('Image1.png',as_gray=True)\n",
    "# The pixel values are in the range [0,1] and we want to convert it to [0,255]\n",
    "grayscale_image = img_as_ubyte(grayscale_image)\n",
    "\n",
    "# Use thresholding to convert the grayscale image into a binary image.\n",
    "thresh = threshold_triangle(grayscale_image)\n",
    "binary_image = grayscale_image < thresh\n",
    "binary_image = binary_image.astype(np.uint8)\n",
    "\n",
    "# Print the dimensions of the color image\n",
    "print(colored_image.shape)\n",
    "\n",
    "# display all images in the same figure\n",
    "fig, ax = plt.subplots(figsize=(16,16), nrows=1, ncols=3)\n",
    "ax[0].imshow(colored_image)\n",
    "ax[0].axis('off')\n",
    "ax[1].imshow(grayscale_image)\n",
    "ax[1].axis('off')\n",
    "ax[2].imshow(binary_image)\n",
    "ax[2].axis('off')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How is the data stored in the colored image `Image1.png`? Notice that `colored_image` in previous step is a numpy array. How is each pixel of the `Image1.png` image represented in array `colored_image`?**\n",
    "\n",
    "`Your answer here...`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting, labelling and coloring connected regions\n",
    "\n",
    "The image `binary_image` contains four shapes, which are **ROI1 (top left), ROI2 (top right), ROI3 (bottom left), ROI4 (bottom right)**, each shape defining a single connected region. The objective is to give different colors to each of the four regions. \n",
    "\n",
    "**Carefully read the following:** \n",
    "\n",
    "For each region, the coordinates of one pixel lying on it are given. This is the seed pixel. Thus, you have `P1, P2, P3, P4` as the coordinates of the seed pixels lying on `ROI1, ROI2, ROI3, ROI4` respectively. You need to use the function [skimage.measure.label()](https://scikit-image.org/docs/stable/api/skimage.measure.html#skimage.measure.label) to first label all pixels in the image. Each pixel is given a unique label, based on which connected region it belongs to. \n",
    "    \n",
    "Thus, the pixels belonging to the same region will have the same label. After that, you need to find the label of the given seed pixel (`P1, P2, P3, P4`) and find other pixels in the image with the same label. Having obtained the four regions, use the `colored_image` to give colors to all the four regions. For your reference, you can refer to the image `expected_image.png` provided along the assignment data. Basically you should have been able to give different colors to each of the four regions.\n",
    "\n",
    "Hint: Remember that each pixel at (i,j) location in `colored_image` has the rgb color defined by the 1x3 numpy array `colored_image[i,j,:]`. \n",
    "E.g. if you want to give red color to pixel at (i,j), you need to execute the following command :\n",
    "\n",
    "`colored_image[i,j,:] = np.array([[255,0,0]])`\n",
    "\n",
    "**1.2. Now you need to use pixel adjacency and connectivity to detect and color each of the four regions in four different colors.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You are given the coordinates of four seed pixels. Each seed pixel lies in exactly one connected region.\n",
    "P1 = np.array([[57],[32]])\n",
    "P2 = np.array([[55],[211]])\n",
    "P3 = np.array([[180],[80]])\n",
    "P4 = np.array([[258],[213]])\n",
    "\n",
    "\"\"\"\n",
    " Label all the connected components in the image. \n",
    " Pay attention to the last parameters `connectivity` in the following function. It can be set to 1 or 2. \n",
    "     connectivity = 1 implies it uses 4-adjacency\n",
    "     connectivity = 2 implies it uses 8-adjacency\n",
    " Therefore use the correct connectivity setting such that you get 4 connected regions. \n",
    "\"\"\"\n",
    "labeled_image, num = measure.label(binary_image, background=False, return_num=True, connectivity=) \n",
    "\n",
    "# 1.2.1 Find the labels of all the seed pixels\n",
    "\n",
    "# 1.2.2 Find the pixels belonging to each of the four labels. These will be the desired regions. There should only be four regions\n",
    "\n",
    "# 1.2.3 Set distinct colors for each of the four regions. You are free to choose colors for the four regions. \n",
    "\n",
    "# Display the colored image\n",
    "fig, ax = plt.subplots(figsize=(8,8), nrows=1, ncols=1)\n",
    "ax.imshow(colored_image)\n",
    "ax.axis('off')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Which of the two parameter values, i.e. connectivity=1 or connectivity=2, led to exactly 4 differently colored regions? Why is it so?**\n",
    "\n",
    "`Your answer here...`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding 4-adjacent and 8-adjacent pixels\n",
    "\n",
    "Continue with the binary version of `Image1.png`, i.e. the `binary_image`. Read the definition of 4-adjacency and 8-adjcency from the lecture notes. \n",
    "\n",
    "**1.3. Implement python functions to compute the 4-adjacent and 8-adjacent pixels of a given pixel in the input binary image. The skeleton of the functions is already provided below. You need to write the logic to obtain the 4-adjacent and 8-adjacent pixels for the given binary image. Note that the set of intensity values is `{1}`. The input to the function will be a Numpy array of dimensions 2x1 and the binary image. The output for both the functions will be the list of 4-adjacent or 8-adjacent pixels. Each pixel will be represented as a Numpy array of 2x1 dimensions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_4_adj_px(P, im):\n",
    "    \"\"\"\n",
    "    P :  2x1 Numpy array of the pixel\n",
    "    im : Binary image\n",
    "    \"\"\"\n",
    "    # The set of intensity values\n",
    "    V = {1}\n",
    "    \n",
    "    adj_px = [P]\n",
    "    # Your logic\n",
    "    \n",
    "    # return the list of adjacent pixels\n",
    "    return adj_px\n",
    "    \n",
    "def find_8_adj_px(P, im):\n",
    "    \"\"\"\n",
    "    P : 2x1 Numpy array of the pixel\n",
    "    im : Binary image\n",
    "    \"\"\"\n",
    "    # The set of intensity values\n",
    "    V = {1}\n",
    "    \n",
    "    adj_px = [P]\n",
    "    # Your logic\n",
    "    \n",
    "    # return the list of adjacent pixels\n",
    "    return adj_px\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.4. Next, use the methods implemented above, to check if two given pixels are 4-adjacent and/or 8-adjacent in the input binary image `binary_image`. Print True or False as output.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The two pixels are the following:\n",
    "P1 = np.array([[58], [207]])\n",
    "P2 = np.array([[57], [208]])\n",
    "\n",
    "# Use the above implemented methods to find 4-adjacent and 8-adjacent pixels\n",
    "adj_pxs = find_4_adj_px(P1, binary_image)\n",
    "\n",
    "# Check if P2 pixel lies in adj_pxs\n",
    "print(np.any(np.all(P2 == adj_pxs, axis=2)))\n",
    "\n",
    "adj_pxs = find_8_adj_px(P1, binary_image)\n",
    "\n",
    "# Check if P2 pixel lies in adj_pxs\n",
    "print(np.any(np.all(P2 == adj_pxs, axis=2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2. Geometric transformations\n",
    "This task involves spatial transformation of the pixels, according to a given transfer mapping.\n",
    "\n",
    "We will being with a study of the curvillinear transformation of an image, which is illustrated by the following step-by-step process. Note that the image transformation is performed by the function `warp2d` contained in the file *custom_warp.py*, which is located in the same folder of this notebook. It is thus important to understand the meaning of the arguments required by that function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load and display the image `cameraman.tif` on the screen. The image size is $256\\times256$ pixels, and usually one assumes a coordinate system where the origin is at the top-corner and the horizontal and vertical coordinates lie within the range $[0,width-1]$ and $[0,height-1]$. However, when dealing with geometric transformations, it is often more convenient to define a new coordinate system such that the image lies in the domain $U\\times V = [-1,1]\\times[-1,1]$. This will have the effect of: _a)_ setting the origin of the coordinate system at the center of the image, _b)_ making the geometric transformation independent from the image size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the domain intervals for horizontal and vertical coordinates (they are parameters required by the function `warp2d`)\n",
    "ubound, vbound = [-1,1], [-1,1]\n",
    "\n",
    "# Load and visualize the image\n",
    "input_image = io.imread(\"cameraman.tif\")\n",
    "plt.imshow(input_image, extent=np.ravel([ubound,vbound]))\n",
    "plt.xlabel('u')\n",
    "plt.ylabel('v')\n",
    "plt.grid(color=[1,1,0.2], alpha=0.3)\n",
    "plt.title(f\"Input image ({input_image.shape[1]}*{input_image.shape[0]} pixels)\" )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose that one wants the above image to be transformed according to the _forward transformation_ $\\tau:U \\times V \\longrightarrow X \\times Y$ defined as follows:\n",
    "\n",
    "$\\tau:\\left\\{\\begin{matrix}\n",
    "x(u,v) & = & u\\\\ \n",
    "y(u,v) & = & v + u^2\n",
    "\\end{matrix}\\right.$\n",
    "\n",
    "The procedure to achieve this is to consider a 2D-array of arbitrary size `output_shape` that will accomodate the pixels of the output image, and for each pixel location $(x,y)$ in it, find its corresponding location in the input image by the _inverse transformation_ $\\tau^{-1}(x,y)$, and finally assign to $(x,y)$ (in the output image) the intensity of the pixel at $\\tau^{-1}(x,y)$ (in the input image). In practice, all this is done internally by `warp2d`, however we must provide the inverse function $\\tau^{-1}$. \n",
    "\n",
    "In this specific example the _inverse function_ $\\tau^{-1}:X \\times Y \\longrightarrow U \\times V$ is easiy obtained by elementary algebra:\n",
    "\n",
    "$\\tau^{-1}:\\left\\{\\begin{matrix}\n",
    "u(x,y) & = & x\\\\ \n",
    "v(x,y) & = & y - x^2\n",
    "\\end{matrix}\\right.$\n",
    "\n",
    "We can now define the inverse transformation $\\tau^{-1}$ in Python code :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The inverse function must take as its argument a numpy array of size Nx2, in which each row represents the [x,y] coordinates\n",
    "# of one of the N points, and it must return an array of the same size containing the transformed [u,v] coordinates\n",
    "# for each point.\n",
    "\n",
    "def inverse_map(xy):\n",
    "    return np.hstack([xy[:,0:1], xy[:,1:2]-xy[:,0:1]**2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we must define suitable values for the size of the output image, and for the bounds of its domain intervals $X$ and $Y$. In principle, all the aforementioned values can be assigned arbitrarily, however the bounds of $X$ and $Y$ are typically chosen in such a way that the whole transformed image is visible, and does not appear cropped. This can be done in several ways. Three possibilities are:\n",
    "\n",
    "_1)_ Manually, by trial and error\n",
    "\n",
    "_2)_ Manually, by using the forward transformation $\\tau$ in order to determine the lower and upper bounds of $x(u,v)$ and $y(u,v)$, when $-1\\leq u,v \\leq 1$\n",
    "\n",
    "_3)_ Automatically, by performing in Pyhton code the calculation in point _2)_.\n",
    "\n",
    "In this example, we choose option _2)_ and we notice that since $x(u,v)=u$ the lower/upper bounds are clearly $[-1,1]$. For $y(u,v)$ it is easy to verify that the lower/upper bounds are reached respectively when $(u,v)=(0,-1)$ and $(u,v)=(1,1)$, and they are $[-1,2]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the bounds for the domain of the output image, as explained above:\n",
    "xbound, ybound = [-1,1], [-1,2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to invoke the warp function `warp2d` to obtain the transformed output image. Note that the function `warp2d` is essentially a wrapper function of [`skimage.transform.warp`](http://scikit-image.org/docs/dev/api/skimage.transform.html#skimage.transform.warp) that accepts domain intervals as its arguments.\n",
    "The syntax for `warp2d` is the following :\n",
    "___\n",
    "*`warp2d`(input_image, inverse_map, ubound=[-1,1], vbound=[-1,1], xbound=[-1,1], ybound=[-1,1], output_shape=None, **kwargs)*\n",
    "\n",
    "__Returns:__ numpy array of size *output_shape* containing the transformed image.\n",
    "\n",
    "__Note:__ The keyword arguments contained in _\\*\\*kwargs_ are the same keyword arguments accepted by [`skimage.transform.warp`](http://scikit-image.org/docs/dev/api/skimage.transform.html#skimage.transform.warp). Please, check the documentation for a complete explanation of the parameters. Some of them are useful to specify, for instance, the _padding_ and the _interpolation_ method to be used.\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the size of the output image:\n",
    "output_shape = [256+128,256]\n",
    "\n",
    "# Invoke the warp function to perform the transformation (see the above instructions for a detailed explanation of the arguments):\n",
    "from custom_warp import warp2d\n",
    "output_image = warp2d(input_image, inverse_map, ubound=ubound, vbound=vbound, xbound=xbound, ybound=ybound, output_shape=output_shape)\n",
    "\n",
    "# Visualize the result:\n",
    "plt.imshow(output_image, extent=np.ravel([xbound,ybound]) )\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid(color=[1,1,0.2], alpha=0.3)\n",
    "plt.title(f\"Output image ({output_image.shape[1]}*{output_image.shape[0]} pixels)\" )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application of Polar-to-Cartesian curvilinear transformation\n",
    "\n",
    "Suppose that a sonar mounted on a ship scans the sea floor by sending acoustic impulses at different angles, and recording the intensity of the returned sound. The directions of the sound impulses are such that the sonar is practically sampling the reflectivity of the sea floor on a polar grid. The samples are then stored into a matrix in which the horizontal axis *u* represents the radial coordinate, and the vertical axis *v* represents the angular coordinate. An example of such an image is found in the file `sonar.png` contained in the folder of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sonar_input = io.imread('sonar.png')\n",
    "\n",
    "plt.imshow(sonar_input)\n",
    "plt.xlabel('u')\n",
    "plt.ylabel('v')\n",
    "plt.title(f\"Input image\" )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above image actually represents an aircraft relic lying on the sea floor, and its appearance looks distorted, especially on the wings. The distortion is caused by the fact that the pixel intensities were originally sampled on a polar grid, but we are now visualizing the image as if its pixels were sampled on a regular Cartesian grid.\n",
    "The main goal will be that of eliminating this distortion by completing the following tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1 Suppose the sonar collected samples on a polar grid at radial coordinates ranging from $0$ to $5$ and angular coordinates ranging from $-60$ to $+60$ degrees. Define two variables containing the respective lower and upper bounds for the coordinates $u$ and $v$ (like _ubound_ and _vbound_ in the previous example), and use them to visualize the image in the new coordinate system (hint: use the _extent_ argument of _imshow_ as in the example in Section 1).**\n",
    "\n",
    "**_Note:_ since numpy trigonometric functions work with radians, it is convenient to express angular coordinates in radians.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the formulas for transforming polar coordinates to Cartesian coordinates are:\n",
    "\n",
    "$\\tau: \\left\\{\\begin{matrix}\n",
    "x(u,v) & = & u\\, \\cos(v)\\\\ \n",
    "y(u,v) & = & u\\, \\sin(v)\n",
    "\\end{matrix}\\right.$\n",
    "\n",
    "Conversely, the Cartesian-to-polar map is given by:\n",
    "\n",
    "$\\tau^{-1}:\\left\\{\\begin{matrix}\n",
    "u(x,y) & = & \\sqrt{x^2+y^2}\\\\ \n",
    "v(x,y) & = & \\mathrm{arctan2}\\left (y,x \\right )\n",
    "\\end{matrix}\\right.$\n",
    "\n",
    "Note that the image _sonar.png_ is defined in the domain $U\\times V$. Hence, following the same reasoning as in the example in the previous Section in this assignment, we seek to obtain an output image defined on a Cartesian domain $X \\times Y$, whose pixel intensities at $(x,y)$ are given by the pixel intensities of `sonar_input` at locations $\\tau^{-1}(x,y)$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2 Define a Python function that evaluates $\\tau^{-1}$. In other words, you have to define a function that takes a $N\\times 2$ numpy array of coordinates \\[x,y\\] and returns an array of the same size containing the corresponging coordinates \\[u,v\\].**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3 Define suitable lower bounds and upper bounds for the $x$ and $y$ coordinates in the output image, and store them respectively in two variables (like _xbound_ and _ybound_ in the previous example). Please, describe very briefly the method you used to obtain the intervals.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4 Choose a suitable size for the output image, and invoke `warp2d` to obtain the transformed output image. Visualize the result by plotting the image. The aircraft should be completely visible and its wings should appear straight. It should be similar to the corrected image attached along this assignment, titled, `sonar_undistorted.png`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aftermath\n",
    "Finally, fill your answers to the following questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How much time did you need to complete this exercise?**\n",
    "\n",
    "`Your answer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Did you experience any problems with the exercise? Was there enough help available? Should this notebook be more (or less) detailed?**\n",
    "\n",
    "`Your answer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "`Enter your references here`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Submission\n",
    "\n",
    "1. Before submitting your work, **check that your notebook (code) runs from scratch** and reproduces all the requested results by clicking on the menu `Kernel -> Restart & Run All`! Also, check that you have answered all the questions written in **bold**.\n",
    "2. Clear all outputs and variables, etc. by click on the menu `Kernel -> Restart & Clear Output`. This may (or will) reduce the file size of your deliverable a lot! \n",
    "3. Rename this Jupyter notebook to **`DIP_PA1_[student number(s)].ipynb`** (e.g. `DIP_PA1_1234567.ipynb` if solo work or `DIP_PA1_1234567-7654321.ipynb` if working in a pair)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
